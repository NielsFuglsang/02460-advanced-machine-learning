# 02460-advanced-machine-learning
Project about privacy in Machine Learning for the course 02460 Advanced Machine Learning Spring 2021.

### Algorithm
![alt text](https://github.com/NielsFuglsang/02460-advanced-machine-learning/blob/main/assets/DLGillustration.PNG?raw=true)

Here we explore the robustness and techniques of deep leakage from gradients (DLG) [[1]](#1). We compare influence of initalization methods and distance measures to explore the convergence rate and speed of single input images. More specifically we hold up the results with what was presented in SAPAG [[2]](#2).


### References
<a id="1">[1]</a> 
Zhu et. al (2020).
"Deep Leakage from Gradients”
Lecture Notes in Computer Science (including sub-series Lecture Notes in Artificial Intelligence and LectureNotes in Bioinformatics), vol. 12500 LNCS, no. NeurIPS,pp. 17–31.

<a id="2">[2]</a> 
Wang et. al. (2020).
“SAPAG: A self-adaptive privacy attackfrom gradients,”.
arXiv.
